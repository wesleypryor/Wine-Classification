\documentclass[letterpaper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{Sweavel}


\input{helper/header} %Formatting, macros, package list, etc
\usepackage{tabularx}
\usepackage{indentfirst}

\title{Wine Classification}
\subtitle{Final Project}
\assignedDate{2016-11-02}
\dueDate{2016-12-09}
\studentName{Wesley Pryor}


\begin{document}

\linenumbers

\section*{Summary of Data and Models}
The Wine Quality Data set is a set of data that contains observations of wines. This data comes form the UCI Machine Learning Repository. The data is divided into two sets.The first data set contains 4898 observations of white wines. The second data set has 1599 different red wines. The data contains 12 attributes. The data has 11 numeric attributes. The numeric attributes are fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol. The twelfth attribute is quality. This part of the data is usually what is trying to be calculated with a regression problem, or it is also determined via classification techniques. I was not sure how this attribute could be treated. I initially thought about treating it just as a normal numeric value. However, I wanted to see how to treat it as a factor.

I decided to tackle this set of data by combining the red and white rows of data. The combines red and white data had 6,497 different type of wines from Portugal. I wanted to classify the wines between red and white.  For this project, I applied classification techniques that we have covered in class throughout the semester to determine the type of wine. The table below describes the techniques applied and the values of accuracy for the respective models. All original models were run with a simple 70-30 split and then cross-validation was applied to the models to determine the mean accuracy for each technique.
\begin{table}[ht]
\caption{Accuracy of Models Using Cross-Validation}
\centering 
\begin{tabular}{c c } 
\hline\hline 
Model & Accuracy \\ [0.5ex]
\hline
R Part Decision Trees & 0.9804547  \\ 
C Tree Decision Trees & 0.9761422  \\
K Nearest Neighbors & 0.9938433  \\
Weighted K nearest Neighbors & 0.9943058  \\
Naive Bayes & 0.9752199  \\ 
Support Vector Machines & 0.9955366  \\
Artificial Neural Networks & 0.9723155  \\[1ex] 
\hline 
\end{tabular}
\end{table}

\subsection*{R Part Decision Trees}
When trying to classify the model. I initially ran the decision tree with the \texttt{rpart} package. This decision tree started with a depth of 4 and and accuracy of 97.6\%. Most of the data in the naive run of the tree was split from the counts of total sulfur dioxides and chlorides. The other factor involved was volatile acidity. Once this model was run, tuning was applied to the complexity parameter, minimum split, and the maximum depth of the trees. 

For the complexity parameters, I decided that the best was to test small values from 0.0 to 0.2. After tuning was run, 0.001 was determined to be the most accurate parameter for the model. From our in class sources, I initially tried to test a minimum split of 5, 10, or 15. However, I was getting the same error rate for all of those values. After looking through the data more, I realized that trying to split with only 5, 10,, or 15 observations would be over-fitting the model and not very ideal. I then tried a splits of 50 125 by 5 and i saw that the errors began to change after 50 splits. 50 was determined to be the best minimum split. The maximum depth was decided from values between 1 and 10. The best tree had a depth of 6. Once complexity parameter, minimum split, and maximum depth were set to the optimal parameters, the accuracy of the rerun decision tree was 98\%. The best decision tree included fixed acidity in splitting the data into the type of wines, along with the other attributes that were in the naive tree. The optimal decision tree was run using 10 fold cross-validation and the mean of those accuracies was 98\%. 

\subsection*{C Tree Decision Trees}
In class this semester, our assignments led us to believe that the best decision tree was used with the \texttt{rpart} calculations instead of the \texttt{ctree} calculations.  I wanted to test the method and determine which method was better for this data. The first attempt of the model split most frequently with density, alcohol, total sulfur dioxides, sulphates and chlorides. Other attributes used in the model were pH, residual sugar, volatile acidity, fixed acidity, and chlorides. With these factors, the model had 69 nodes and the model has a depth of 8. The initial accuracy was 98.1\% The model was tuned for the minimum split and the maximum depth. It was tested for values of minimum split at 50, 100, and 150. The optimal value was 50. At maximum depth, values between 1 and 10 were tested. The best depth was calculated to be 10. The optimal tree was tested with these values and had an accuracy of 98.3\%.  The tree looked similar to the first run of a decision tree. However, the overall depth was increased by one. Once cross-validation was done with this model, the average accuracy was much lower at 97.6\%. I was suprised to see this. However, I believe that this was done from splits that had more data that was had similar values in certain categories. Overall after cross-validation of both methods, the \texttt{rpart} calculations were proven to be better for this data.

\subsection*{K nearest neighbors}
K nearest neighbors was standardized initially and was calculated with 3 nearest neighbors. This trial resulted in a 99.2\% accuracy. The trial was then tuned with nearest neighbors of 2 through 20. The method was also tuned with three different  types of sampling: cross-validation, fixed sampling, and bootstrapping. The best nearest neighbors for cross-validation was 5. Fixed sampling had an optimal nearest neighbor count of 11. Bootstrapping was 12. I tested the model with all the values of k that were determined in the sampling and then cross-validated. Through tuning and cross-validation, the overall average accuracy was 99.3\%

\subsection*{Weighted K Nearest Neighbors}
I wanted to test to see how the kernels of K nearest neighbors affected the values of the model. The first trial was with a triangular kernel and the accuracy was 99.2\%. I tested all eight kernels available. I expected that the optimal kernel would perform better than my first trial. After testing, the inverted kernel was determined to have minimal errors with 3 nearest neighbors. The accuracy was run once for a 99.2\% accuracy. cross-validation resulted in a mean 99.4\% accuracy.

\subsection*{Naive Bayes}
Naive Bayes has the assumption of normality. I had to first test the normality of all attributes of the data. The numerical values of the data were not normal. At this point I tried to see if I could treat the quality as a factor instead of a numerical value. All models had obvious curvature in all of the 	Q-Q Plots. Also, all Shapiro-Wilk tests returned the smallest possible p-values for a normal distribution. The data had to be cut to assume normality with the \texttt{mycut} function that was developed in this semester. The attributes that needed normalizing were cut into 10 factor levels. Due to time constraints, I was not able to test multiple cuts. I wish that I had tested multiple cuts to find the best cut for the model. Due to the size of the data, I tried 10 and felt like this was a good estimate. After normalization, the first attempt at the model had a 97.8\% accuracy. The model then underwent 10 fold cross-validation and had a mean accuracy of 97.5\%.

\subsection*{Support Vector Machines}
Before we could start Support Vector Machines, I removed the the combined database that was adjusted for normality. Then I recommitted the combined set of data and ran the support vector machines. The trial support vector machines had an accuracy of 99.6\% percent. The tuning then was tested for gamma and costs parameters. The first time that I tuned the function, the upper extremities were the best parameters. I then attempted to tune again with the upper extremities as the lower boundaries of the function. The second attempt at tuning the function resulted in the lower bounds, the original upper extremities, were the optimal gamma and cost. Therefore these values were used and the model after tuning was 99.6\%.

\subsection*{Artificial Neural Networks}
The neural networks was run using the \texttt{nnet} package. The first time it was run with 2 levels and a maximum iteration at 100. The first accuracy was at 99.8\% I changed the iterations to 500 and the size of the neural network to 10 hidden layers. During the run of the neural network command. The first iterations stopped at 230 and the accuracy was 99.7\%. The network was tuned with sizes of hidden layers between 1 and 15. Also the tuning function was applied with controls of 5 repeats, cross validation sampling, and 10 crosses. The tuning function ran cross validation and resulted in 97.2\% accuracy. I was shocked at this low value considering the first values that i got that were extremely high. I think that my first guess of 10 layers was the best for the model and the sum from all other sizes of hidden layers caused the value to overall be lower.

\section*{Conclusion}
Overall, Support Vector Machines were the optimal model for this data. Neural Networks and the C Tree Decision Trees were the overall worst models for the data. As I was working with the data, I really did not consider the chemical make up of a red wine and the chemical makeup of a white wine. I just wanted to run the data to see how it behaved on it's own. After looking more into the data, there are certain key attributes that really determine the model. Alcohol, density, total sulfur dioxides, free sulfur dioxides, chlorides, and pH were the best attributes that could determine the type of wine. Models where these were these were used instead of the whole data could probably perform at similar accuracies to the models run with all attributes. 

\section*{Code Appendix}
\begin{Schunk}
\begin{Sinput}
set.seed(1506)
# Importing Data Sets
Red_Wine <- read.csv("~/Downloads/winequality-red.csv", sep = ";")
\end{Sinput}
\begin{Soutput}
Warning in file(file, "rt"): cannot open file '/Users/wdpryor1994/Downloads/winequality-red.csv': No such file or directory
\end{Soutput}
\begin{Soutput}
Error in file(file, "rt"): cannot open the connection
\end{Soutput}
\begin{Sinput}
White_Wine <- read.csv("~/Downloads/winequality-white.csv", sep = ";")
\end{Sinput}
\begin{Soutput}
Warning in file(file, "rt"): cannot open file '/Users/wdpryor1994/Downloads/winequality-white.csv': No such file or directory
\end{Soutput}
\begin{Soutput}
Error in file(file, "rt"): cannot open the connection
\end{Soutput}
\begin{Sinput}
# Adding the Type of Wine to the data set.
Red <- c(rep("red", nrow(Red_Wine)))
\end{Sinput}
\begin{Soutput}
Error in nrow(Red_Wine): object 'Red_Wine' not found
\end{Soutput}
\begin{Sinput}
White <- c(rep("white", nrow(White_Wine)))
\end{Sinput}
\begin{Soutput}
Error in nrow(White_Wine): object 'White_Wine' not found
\end{Soutput}
\begin{Sinput}
Red_Wine <- cbind(Red_Wine, Red)
\end{Sinput}
\begin{Soutput}
Error in cbind(Red_Wine, Red): object 'Red_Wine' not found
\end{Soutput}
\begin{Sinput}
White_Wine <- cbind(White_Wine, White)
\end{Sinput}
\begin{Soutput}
Error in cbind(White_Wine, White): object 'White_Wine' not found
\end{Soutput}
\begin{Sinput}
# Changing the type of wine columns
colnames(Red_Wine)[13] <- "wine.type"
\end{Sinput}
\begin{Soutput}
Error in colnames(Red_Wine)[13] <- "wine.type": object 'Red_Wine' not found
\end{Soutput}
\begin{Sinput}
colnames(White_Wine)[13] <- "wine.type"
\end{Sinput}
\begin{Soutput}
Error in colnames(White_Wine)[13] <- "wine.type": object 'White_Wine' not found
\end{Soutput}
\begin{Sinput}
# Combining Both Red and White Wines into one data set
Total_Wine <- rbind(Red_Wine, White_Wine)
\end{Sinput}
\begin{Soutput}
Error in rbind(Red_Wine, White_Wine): object 'Red_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$quality <- as.factor(Total_Wine$quality)
\end{Sinput}
\begin{Soutput}
Error in is.factor(x): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# Item that will be predicted with the classification.
Wine.Type <- Total_Wine$wine.type
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# Splitting Data for testing purposes
Wine.Split <- splitdata(Total_Wine, 0.7)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Wine.train <- Wine.Split$traindata
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.Split' not found
\end{Soutput}
\begin{Sinput}
Wine.test <- Wine.Split$testdata
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.Split' not found
\end{Soutput}
\begin{Sinput}
# Decision Trees to determine the type of wine, red or white.
wine.tree <- rpart(wine.type ~ ., data = Wine.train)
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
plot.wine.tree <- fancyRpartPlot(wine.tree, palettes = c("Reds", "Greys"))
\end{Sinput}
\begin{Soutput}
Error in fancyRpartPlot(wine.tree, palettes = c("Reds", "Greys")): object 'wine.tree' not found
\end{Soutput}
\begin{Sinput}
pred.tree <- predict(wine.tree, newdata = Wine.test, type = "class")
\end{Sinput}
\begin{Soutput}
Error in predict(wine.tree, newdata = Wine.test, type = "class"): object 'wine.tree' not found
\end{Soutput}
\begin{Sinput}
# Analysis of the decision tree
tree.conf <- confusion(Wine.test$wine.type, pred.tree, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
tree.conf
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'tree.conf' not found
\end{Soutput}
\begin{Sinput}
# Tuning decision tree
tree.tune <- tune.rpart(wine.type ~ ., data = Total_Wine, cp = seq(0, 0.5, 0.001), 
    maxdepth = 1:10)
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
tree.tune.2 <- tune.rpart(wine.type ~ ., data = Total_Wine, minsplit = seq(50, 
    125, 5))
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
plot(tree.tune)
\end{Sinput}
\begin{Soutput}
Error in plot(tree.tune): object 'tree.tune' not found
\end{Soutput}
\begin{Sinput}
plot(tree.tune.2)
\end{Sinput}
\begin{Soutput}
Error in plot(tree.tune.2): object 'tree.tune.2' not found
\end{Soutput}
\begin{Sinput}
# The best parameters for the rpart decision tree
tree.tune$best.parameters
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'tree.tune' not found
\end{Soutput}
\begin{Sinput}
tree.tune.2$best.parameters
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'tree.tune.2' not found
\end{Soutput}
\begin{Sinput}
# Optimal rpart tree
tree.2 <- rpart(wine.type ~ ., data = Wine.train, control = rpart.control(minsplit = tree.tune.2$best.parameters$minsplit, 
    cp = tree.tune$best.parameters$cp, maxdepth = tree.tune$best.parameters$maxdepth))
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
fancyRpartPlot(tree.2, palettes = c("Reds", "Greys"))
\end{Sinput}
\begin{Soutput}
Error in fancyRpartPlot(tree.2, palettes = c("Reds", "Greys")): object 'tree.2' not found
\end{Soutput}
\begin{Sinput}
pred.tree.2 <- predict(tree.2, newdata = Wine.test, type = "class")
\end{Sinput}
\begin{Soutput}
Error in predict(tree.2, newdata = Wine.test, type = "class"): object 'tree.2' not found
\end{Soutput}
\begin{Sinput}
# Analysis of optimal decision tree with cross validation
tree.conf.2 <- confusion(Wine.test$wine.type, pred.tree.2, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
tree.conf.2
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'tree.conf.2' not found
\end{Soutput}
\begin{Sinput}
kflval <- function(k, data) {
    folds = createfolds(nrow(data), k)
    accvector = 1:k
    for (k in 1:k) {
        temptrain = data[folds != k, ]
        temptest = data[folds == k, ]
        temptree = rpart(wine.type ~ ., data = temptrain, control = rpart.control(minsplit = tree.tune.2$best.parameters$minsplit, 
            cp = tree.tune$best.parameters$cp, maxdepth = tree.tune$best.parameters$maxdepth))
        temppred = predict(temptree, newdata = temptest, type = "class")
        analysis = confusion(temptest$wine.type, temppred)
        accvector[k] = analysis$acc
    }
    return(mean(accvector))
}

kflval(10, Total_Wine)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# Ctree
wine.ctree <- ctree(wine.type ~ ., data = Wine.train)
\end{Sinput}
\begin{Soutput}
Error in terms.formula(formula, data = data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
plot(wine.ctree)
\end{Sinput}
\begin{Soutput}
Error in plot(wine.ctree): object 'wine.ctree' not found
\end{Soutput}
\begin{Sinput}
# Prediction and confusion of ctree decision tree
pred.tree.3 <- predict(wine.ctree, newdata = Wine.test)
\end{Sinput}
\begin{Soutput}
Error in predict(wine.ctree, newdata = Wine.test): object 'wine.ctree' not found
\end{Soutput}
\begin{Sinput}
wine.ctree.conf <- confusion(Wine.test$wine.type, pred.tree.3, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
wine.ctree.conf
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'wine.ctree.conf' not found
\end{Soutput}
\begin{Sinput}
# Testing minsplit and maxdepth of ctree
maxdepth <- 1:10
# 50 minsplit
accvec <- NULL
for (i in 1:length(maxdepth)) {
    temp.tree <- ctree(wine.type ~ ., data = Wine.train, controls = ctree_control(minsplit = 50, 
        maxdepth = maxdepth[i]))
    temp.pred <- predict(temp.tree, newdata = Wine.test)
    temp.conf <- confusion(Wine.test$wine.type, temp.pred, costs = NULL)
    accvec[i] <- temp.conf$acc
}
\end{Sinput}
\begin{Soutput}
Error in terms.formula(formula, data = data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
# 100 misplit
accvec.2 <- NULL
for (i in 1:length(maxdepth)) {
    temp.tree <- ctree(wine.type ~ ., data = Wine.train, controls = ctree_control(minsplit = 100, 
        maxdepth = maxdepth[i]))
    temp.pred <- predict(temp.tree, newdata = Wine.test)
    temp.conf <- confusion(Wine.test$wine.type, temp.pred, costs = NULL)
    accvec.2[i] <- temp.conf$acc
}
\end{Sinput}
\begin{Soutput}
Error in terms.formula(formula, data = data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
# 150 minsplit
accvec.3 <- NULL
for (i in 1:length(maxdepth)) {
    temp.tree <- ctree(wine.type ~ ., data = Wine.train, controls = ctree_control(minsplit = 150, 
        maxdepth = maxdepth[i]))
    temp.pred <- predict(temp.tree, newdata = Wine.test)
    temp.conf <- confusion(Wine.test$wine.type, temp.pred, costs = NULL)
    accvec.3[i] <- temp.conf$acc
}
\end{Sinput}
\begin{Soutput}
Error in terms.formula(formula, data = data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
# Creating information matrix
max.depth <- as.vector(c(rep(list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 3)))
min.split <- as.vector(c(rep(list(50, 100, 150), 10)))
accuracy <- as.vector(rbind(accvec, accvec.2, accvec.3))
acc.matrix <- cbind(max.depth, min.split, accuracy)
colnames(acc.matrix)[1] <- "maxdepth"
colnames(acc.matrix)[2] <- "misplit"
colnames(acc.matrix)[3] <- "accuracy"
\end{Sinput}
\begin{Soutput}
Error in `colnames<-`(`*tmp*`, value = c("maxdepth", "misplit", "accuracy": length of 'dimnames' [2] not equal to array extent
\end{Soutput}
\begin{Sinput}
which.max(accuracy)
\end{Sinput}
\begin{Soutput}
integer(0)
\end{Soutput}
\begin{Sinput}
acc.matrix[which.max(accuracy), ]
\end{Sinput}
\begin{Soutput}
     maxdepth misplit
\end{Soutput}
\begin{Sinput}
# Testing Optimal Ctree
opt.ctree <- ctree(wine.type ~ ., data = Wine.train, controls = ctree_control(minsplit = 50, 
    maxdepth = 10))
\end{Sinput}
\begin{Soutput}
Error in terms.formula(formula, data = data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
plot(opt.ctree)
\end{Sinput}
\begin{Soutput}
Error in plot(opt.ctree): object 'opt.ctree' not found
\end{Soutput}
\begin{Sinput}
opt.pred <- predict(opt.ctree, newdata = Wine.test)
\end{Sinput}
\begin{Soutput}
Error in predict(opt.ctree, newdata = Wine.test): object 'opt.ctree' not found
\end{Soutput}
\begin{Sinput}
opt.conf <- confusion(Wine.test$wine.type, opt.pred, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
opt.conf
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'opt.conf' not found
\end{Soutput}
\begin{Sinput}
# Cross Validation with C Tree
ctree.kflval <- function(k, data) {
    folds = createfolds(nrow(data), k)
    accvector = 1:k
    for (k in 1:k) {
        temptrain = data[folds != k, ]
        temptest = data[folds == k, ]
        temptree = ctree(wine.type ~ ., data = temptrain, controls = ctree_control(minsplit = 50, 
            maxdepth = 10))
        temppred = predict(temptree, newdata = temptest)
        analysis = confusion(temptest$wine.type, temppred)
        accvector[k] = analysis$acc
    }
    return(mean(accvector))
}

ctree.kflval(10, Total_Wine)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# K nearest neighbors standardize the data
x = Total_Wine[, 1:11]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
xbar = apply(x, 2, mean)
\end{Sinput}
\begin{Soutput}
Error in apply(x, 2, mean): object 'x' not found
\end{Soutput}
\begin{Sinput}
xbarMat = cbind(rep(1, nrow(Total_Wine))) %*% xbar
\end{Sinput}
\begin{Soutput}
Error in nrow(Total_Wine): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
s = apply(x, 2, sd)
\end{Sinput}
\begin{Soutput}
Error in apply(x, 2, sd): object 'x' not found
\end{Soutput}
\begin{Sinput}
sMat = cbind(rep(1, nrow(Total_Wine))) %*% s
\end{Sinput}
\begin{Soutput}
Error in nrow(Total_Wine): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
z = (x - xbarMat)/sMat
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'x' not found
\end{Soutput}
\begin{Sinput}
# K nearest neighbors sampling the data
z.split <- sample(nrow(z), round(nrow(z) * 0.7, 0))
\end{Sinput}
\begin{Soutput}
Error in nrow(z): object 'z' not found
\end{Soutput}
\begin{Sinput}
z.train <- z[z.split, ]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'z' not found
\end{Soutput}
\begin{Sinput}
z.test <- z[-z.split, ]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'z' not found
\end{Soutput}
\begin{Sinput}
# K nearest neighbors Test
wine.knn <- knn(train = z.train, test = z.test, k = 3, cl = Wine.Type[z.split])
\end{Sinput}
\begin{Soutput}
Error in as.matrix(train): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
confusion(Wine.Type[-z.split], wine.knn, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.Type' not found
\end{Soutput}
\begin{Sinput}
# Finding the optimal k for K nearest neighbors
x <- Total_Wine[, -13]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
y <- Total_Wine[, 13]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
knn.tune <- tune.knn(x, y, k = seq(2, 20, 1), tunecontrol = tune.control(sampling = "cross"))
\end{Sinput}
\begin{Soutput}
Error in tune("knn.wrapper", train.x = x, train.y = y, ranges = ranges, : object 'y' not found
\end{Soutput}
\begin{Sinput}
knn.tune.2 <- tune.knn(x, y, k = seq(2, 20, 1), tunecontrol = tune.control(sampling = "fix"))
\end{Sinput}
\begin{Soutput}
Error in tune("knn.wrapper", train.x = x, train.y = y, ranges = ranges, : object 'y' not found
\end{Soutput}
\begin{Sinput}
knn.tune.3 <- tune.knn(x, y, k = seq(2, 20, 1), tunecontrol = tune.control(sampling = "boot"))
\end{Sinput}
\begin{Soutput}
Error in tune("knn.wrapper", train.x = x, train.y = y, ranges = ranges, : object 'y' not found
\end{Soutput}
\begin{Sinput}
plot(knn.tune)
\end{Sinput}
\begin{Soutput}
Error in plot(knn.tune): object 'knn.tune' not found
\end{Soutput}
\begin{Sinput}
plot(knn.tune.2)
\end{Sinput}
\begin{Soutput}
Error in plot(knn.tune.2): object 'knn.tune.2' not found
\end{Soutput}
\begin{Sinput}
plot(knn.tune.3)
\end{Sinput}
\begin{Soutput}
Error in plot(knn.tune.3): object 'knn.tune.3' not found
\end{Soutput}
\begin{Sinput}
knn.tune$best.parameters$k
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'knn.tune' not found
\end{Soutput}
\begin{Sinput}
knn.tune.2$best.parameters$k
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'knn.tune.2' not found
\end{Soutput}
\begin{Sinput}
knn.tune.3$best.parameters$k
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'knn.tune.3' not found
\end{Soutput}
\begin{Sinput}
wine.knn.2 <- knn(train = z.train, test = z.test, k = knn.tune$best.parameters$k, 
    cl = Wine.Type[z.split])
\end{Sinput}
\begin{Soutput}
Error in as.matrix(train): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
wine.knn.3 <- knn(train = z.train, test = z.test, k = knn.tune.2$best.parameters$k, 
    cl = Wine.Type[z.split])
\end{Sinput}
\begin{Soutput}
Error in as.matrix(train): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
wine.knn.4 <- knn(train = z.train, test = z.test, k = knn.tune.3$best.parameters$k, 
    cl = Wine.Type[z.split])
\end{Sinput}
\begin{Soutput}
Error in as.matrix(train): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
wine.knn.2.conf <- confusion(Wine.Type[-z.split], wine.knn.2, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.Type' not found
\end{Soutput}
\begin{Sinput}
wine.knn.3.conf <- confusion(Wine.Type[-z.split], wine.knn.3, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.Type' not found
\end{Soutput}
\begin{Sinput}
wine.knn.4.conf <- confusion(Wine.Type[-z.split], wine.knn.4, costs = NULL)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.Type' not found
\end{Soutput}
\begin{Sinput}
# Cross Validation for K nearest neighbors
cross.knn <- knn.cv(train = z, cl = y, k = knn.tune$best.parameters$k)
\end{Sinput}
\begin{Soutput}
Error in as.matrix(train): object 'z' not found
\end{Soutput}
\begin{Sinput}
cross.knn.conf <- confusion(Wine.Type, cross.knn)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.Type' not found
\end{Soutput}
\begin{Sinput}
# Weighted K nearest neighbors
z <- cbind(z, Wine.Type)
\end{Sinput}
\begin{Soutput}
Error in cbind(z, Wine.Type): object 'z' not found
\end{Soutput}
\begin{Sinput}
z.train <- z[z.split, ]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'z' not found
\end{Soutput}
\begin{Sinput}
z.test <- z[-z.split, ]
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'z' not found
\end{Soutput}
\begin{Sinput}
weight.knn.wine <- kknn(Wine.Type ~ ., train = z.train, test = z.test, k = knn.tune$best.parameters$k, 
    kernel = "triangular")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
pred.weight <- predict(weight.knn.wine, newdata = z.test)
\end{Sinput}
\begin{Soutput}
Error in predict(weight.knn.wine, newdata = z.test): object 'weight.knn.wine' not found
\end{Soutput}
\begin{Sinput}
confusion(z.test$Wine.Type, pred.weight)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'z.test' not found
\end{Soutput}
\begin{Sinput}
# Finding the best kernels
tune.kknn <- function(K, kernels) {
    acc <- NULL
    for (i in 1:length(K)) {
        temp.model <- kknn(Wine.Type ~ ., train = z.train, test = z.test, k = i, 
            kernel = kernels, distance = 2)
        pred.temp <- predict(temp.model, newdata = z.test)
        temp.conf <- confusion(z.test$Wine.Type, pred.temp)
        acc[i] <- temp.conf$acc
    }
    opt.k <- which.max(acc)
    opt.acc <- acc[opt.k]
    return(list(k = opt.k, accuracy = opt.acc))
}
K <- 2:30
rect <- tune.kknn(K, "rectangular")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
triangle <- tune.kknn(K, "triangular")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
ep <- tune.kknn(K, "epanechnikov")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
biw <- tune.kknn(K, "biweight")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
triw <- tune.kknn(K, "triweight")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
cosine <- tune.kknn(K, "cos")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
invert <- tune.kknn(K, "inv")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
gauss <- tune.kknn(K, "gaussian")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
rank.weight <- tune.kknn(K, "rank")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
optimal <- tune.kknn(K, "optimal")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
weighted.accuracy <- list(rect$accuracy, triangle$accuracy, ep$accuracy, biw$accuracy, 
    triw$accuracy, cosine$accuracy, invert$accuracy, gauss$accuracy, rank.weight$accuracy, 
    optimal$accuracy)
\end{Sinput}
\begin{Soutput}
Error in rect$accuracy: object of type 'closure' is not subsettable
\end{Soutput}
\begin{Sinput}
weighted.k <- list(rect$k, triangle$k, ep$k, biw$k, triw$k, cosine$k, invert$k, 
    gauss$k, rank.weight$k, optimal$k)
\end{Sinput}
\begin{Soutput}
Error in rect$k: object of type 'closure' is not subsettable
\end{Soutput}
\begin{Sinput}
opt.weights <- cbind(weighted.k, weighted.accuracy)
\end{Sinput}
\begin{Soutput}
Error in cbind(weighted.k, weighted.accuracy): object 'weighted.k' not found
\end{Soutput}
\begin{Sinput}
opt.kknn <- kknn(Wine.Type ~ ., train = z.train, test = z.test, k = 3, kernel = "inv")
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'z.train' not found
\end{Soutput}
\begin{Sinput}
pred.kknn <- predict(opt.kknn, newdata = z.test)
\end{Sinput}
\begin{Soutput}
Error in predict(opt.kknn, newdata = z.test): object 'opt.kknn' not found
\end{Soutput}
\begin{Sinput}
confusion(z.test$Wine.Type, pred.kknn)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'z.test' not found
\end{Soutput}
\begin{Sinput}
# Cross Validation fro Weighted K nearest neighbors
kknn.kflval <- function(k, data) {
    folds = createfolds(nrow(data), k)
    accvector = 1:k
    for (k in 1:k) {
        temptrain = data[folds != k, ]
        temptest = data[folds == k, ]
        tempmodel = kknn(Wine.Type ~ ., train = temptrain, test = temptest, 
            k = 3, kernel = "inv")
        temppred = predict(tempmodel, newdata = temptest, type = "raw")
        analysis = confusion(temptest$Wine.Type, temppred)
        accvector[k] = analysis$acc
    }
    return(mean(accvector))
}

kknn.kflval(10, z)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'z' not found
\end{Soutput}
\begin{Sinput}
# Investigating qualities of data Naive Bayes
normality(Wine.train$fixed.acidity)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$volatile.acidity)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$citric.acid)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$residual.sugar)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$chlorides)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$free.sulfur.dioxide)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$total.sulfur.dioxide)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$density)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$pH)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$sulphates)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
normality(Wine.train$alcohol)
\end{Sinput}
\begin{Soutput}
Error in hist(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
# Discretizing data
Total_Wine$fixed.acidity <- mycut(Total_Wine$fixed.acidity, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$volatile.acidity <- mycut(Total_Wine$volatile.acidity, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$citric.acid <- mycut(Total_Wine$citric.acid, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$residual.sugar <- mycut(Total_Wine$residual.sugar, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$chlorides <- mycut(Total_Wine$chlorides, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$free.sulfur.dioxide <- mycut(Total_Wine$free.sulfur.dioxide, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$total.sulfur.dioxide <- mycut(Total_Wine$total.sulfur.dioxide, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$density <- mycut(Total_Wine$density, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$pH <- mycut(Total_Wine$pH, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$sulphates <- mycut(Total_Wine$sulphates, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$alcohol <- mycut(Total_Wine$alcohol, 10)
\end{Sinput}
\begin{Soutput}
Error in quantile(x, probs = seq(0, 1, length.out = n + 1)): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine$quality <- as.factor(Total_Wine$quality)
\end{Sinput}
\begin{Soutput}
Error in is.factor(x): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Wine.Split <- splitdata(Total_Wine, 0.7)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Wine.train <- Wine.Split$traindata
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.Split' not found
\end{Soutput}
\begin{Sinput}
Wine.test <- Wine.Split$testdata
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.Split' not found
\end{Soutput}
\begin{Sinput}
# Applying Naive Bayes
nb.wine <- naiveBayes(wine.type ~ ., data = Wine.train)
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
pred.nb.wine <- predict(nb.wine, newdata = Wine.test, type = "class")
\end{Sinput}
\begin{Soutput}
Error in predict(nb.wine, newdata = Wine.test, type = "class"): object 'nb.wine' not found
\end{Soutput}
\begin{Sinput}
nb.conf <- confusion(Wine.test$wine.type, pred.nb.wine)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
# Cross Validation
nb.kflval <- function(k, data) {
    folds = createfolds(nrow(data), k)
    accvector = 1:k
    for (k in 1:k) {
        temptrain = data[folds != k, ]
        temptest = data[folds == k, ]
        tempmodel = naiveBayes(wine.type ~ ., data = temptrain)
        temppred = predict(tempmodel, newdata = temptest, type = "class")
        analysis = confusion(temptest$wine.type, temppred)
        accvector[k] = analysis$acc
    }
    return(mean(accvector))
}

nb.kflval(10, Total_Wine)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# Support Vector Machines
rm(Total_Wine)
\end{Sinput}
\begin{Soutput}
Warning in rm(Total_Wine): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
Total_Wine <- rbind(Red_Wine, White_Wine)
\end{Sinput}
\begin{Soutput}
Error in rbind(Red_Wine, White_Wine): object 'Red_Wine' not found
\end{Soutput}
\begin{Sinput}
wine.svm <- svm(wine.type ~ ., data = Wine.train, kernel = "linear", cost = 1, 
    scale = T)
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
pred.svm <- predict(wine.svm, newdata = Wine.test)
\end{Sinput}
\begin{Soutput}
Error in predict(wine.svm, newdata = Wine.test): object 'wine.svm' not found
\end{Soutput}
\begin{Sinput}
svm.conf <- confusion(Wine.test$wine.type, pred.svm)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
plot(wine.type ~ ., wine.svm, data = Wine.train)
\end{Sinput}
\begin{Soutput}
Error in plot(wine.type ~ ., wine.svm, data = Wine.train): object 'wine.svm' not found
\end{Soutput}
\begin{Sinput}
# Tuning gamma and cost on SVM
svm.tuning <- tune.svm(wine.type ~ ., data = Total_Wine, gamma = 10^(-6:-3), 
    cost = 10^(1:2))
\end{Sinput}
\begin{Soutput}
Error in tune("svm", train.x = x, data = data, ranges = ranges, ...): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
svm.tuning$best.parameters
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'svm.tuning' not found
\end{Soutput}
\begin{Sinput}
svm.tuning.2 <- tune.svm(wine.type ~ ., data = Total_Wine, gamma = 10^(-3:2), 
    cost = 10^(2:4))
\end{Sinput}
\begin{Soutput}
Error in tune("svm", train.x = x, data = data, ranges = ranges, ...): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# Optmal SVM
opt.svm <- svm(wine.type ~ ., data = Wine.train, kernel = "linear", gamma = svm.tuning.2$best.parameters$gamma, 
    cost = svm.tuning.2$best.parameters$cost)
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.train' not found
\end{Soutput}
\begin{Sinput}
# Cross Validation of SVM
svm.kflval <- function(k, data) {
    folds = createfolds(nrow(data), k)
    accvector = 1:k
    for (k in 1:k) {
        temptrain = data[folds != k, ]
        temptest = data[folds == k, ]
        tempmodel = svm(wine.type ~ ., data = Wine.train, kernel = "linear", 
            gamma = svm.tuning.2$best.parameters$gamma, cost = svm.tuning.2$best.parameters$cost)
        temppred = predict(tempmodel, newdata = temptest)
        analysis = confusion(temptest$wine.type, temppred)
        accvector[k] = analysis$acc
    }
    return(mean(accvector))
}

svm.kflval(10, Total_Wine)
\end{Sinput}
\begin{Soutput}
Error in nrow(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
# Neural Networks
wine.nnet <- nnet(wine.type ~ ., data = Wine.test, size = 10, linout = FALSE, 
    maxit = 500)
\end{Sinput}
\begin{Soutput}
Error in eval(expr, envir, enclos): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
pred.nnet <- predict(wine.nnet, newdata = Wine.test, type = "class")
\end{Sinput}
\begin{Soutput}
Error in predict(wine.nnet, newdata = Wine.test, type = "class"): object 'wine.nnet' not found
\end{Soutput}
\begin{Sinput}
confusion(Wine.test$wine.type, pred.nnet)
\end{Sinput}
\begin{Soutput}
Error in table(true, pred): object 'Wine.test' not found
\end{Soutput}
\begin{Sinput}
nnet.tuning <- tune.nnet(wine.type ~ ., data = Total_Wine, size = 1:15, trace = FALSE, 
    tunecontrol = tune.control(nrepeat = 5, sampling = "cross", cross = 10, 
        ))
\end{Sinput}
\begin{Soutput}
Error in is.data.frame(data): object 'Total_Wine' not found
\end{Soutput}
\begin{Sinput}
1 - mean(nnet.tuning$performances$error)
\end{Sinput}
\begin{Soutput}
Error in mean(nnet.tuning$performances$error): object 'nnet.tuning' not found
\end{Soutput}
\begin{Sinput}
# Plotting Neaural Networks import the function from Github
library(devtools)
source_url("https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r")
\end{Sinput}
\begin{Soutput}
SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef
\end{Soutput}
\begin{Sinput}
plot.nnet(wine.nnet)
\end{Sinput}
\begin{Soutput}
Loading required package: scales
\end{Soutput}
\begin{Soutput}

Attaching package: 'scales'
\end{Soutput}
\begin{Soutput}
The following object is masked from 'package:kernlab':

    alpha
\end{Soutput}
\begin{Soutput}
Error in match(x, table, nomatch = 0L): object 'wine.nnet' not found
\end{Soutput}
\end{Schunk}
\end{document}
